{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2fv4zlR0Vt74/6mwu6WdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raulbenitez/PRML_exploratory_analysis/blob/main/PRML_Exercise_Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE: DIMENSIONALITY REDUCTION "
      ],
      "metadata": {
        "id": "8bX7q-E9I7AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1:  Load a UCI database:\n",
        "\n",
        "Obtain a multivariate dataset with **numerical** features for classification problems from the UCI Machine Learning Repository ($n$ observations, $d$ features). \n",
        "\n",
        "http://archive.ics.uci.edu/ml/ \n",
        "\n",
        "If the database is aimed for classification, make sure to keep only the features and remove the class labels. The resulting matrix should be of size $m \\times d$ with $m$ observations and $d$ attributes/variables/features. \n",
        "\n",
        "If the database contains NaNs or missing values, apply a data imputation strategy (remove observations, replace by variable mean, multivariate imputation, etc).\n",
        "\n",
        "https://scikit-learn.org/stable/modules/impute.html"
      ],
      "metadata": {
        "id": "h4eMohZAG1I3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zzmfw9mfG9g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2:  Data Exploration \n",
        "\n",
        "a) Obtain the summary statistics for all the attributes \n",
        "\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html\n",
        "\n",
        "b) Visualize all the variables in the data set using a scatter plot matrix. \n",
        "\n",
        "https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
        "\n",
        "c) Use the class labels of the observations in order to represent the data as a class-grouped scatter plot matrix. \n",
        "\n",
        "d) Compute the pairwise linear correlations between variables and represent the results as a correlation plot/heatmap. \n",
        "\n",
        "https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
        "\n"
      ],
      "metadata": {
        "id": "WDpHBp3vG9pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Iaqav3dHguc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Dimensionality Reduction with PCA\n",
        "\n",
        "a) Use the correlation heatmap generated in task 2d in order to identify which variables present a significant correlation.\n",
        "\n",
        "b) Perform a PCA decomposition of the data. Plot the resulting eigenvalues in decreasing order and select how many of them are needed in order to represent a 95\\% of the variance in the data (scree plot). \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
        "\n",
        "c) Generate a table with the obtained eigenvectors and eigenvalues.\n",
        "\n",
        "d) Project and represent the original data into the reduced dimensionality PCA space. \n"
      ],
      "metadata": {
        "id": "iT38XtzOHg1-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ioll_L39IV01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Visualization of high-dimensional data\n",
        "\n",
        "a) Apply Multidimensional Scaling (MDS) in order to project the d-dimensional data in a 2-d space and graphically represent the result\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html\n",
        "\n",
        "b) Apply t-SNE in order to project the d-dimensional data in a 2-d space and graphically represent the result\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
        "\n",
        "c) Compare the results obtained with MDS and t-SNE\n"
      ],
      "metadata": {
        "id": "IDz4xdrjIV8n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Tc_M6wHKCXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Feature Selection\n",
        "\n",
        "a) Apply a univariate feature selection based on variable variances\n",
        "\n",
        "https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
        "\n",
        "b) Apply a multivariate feature selection using the Recursive Feature Elimination method\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py"
      ],
      "metadata": {
        "id": "2QFLr08iKCeX"
      }
    }
  ]
}